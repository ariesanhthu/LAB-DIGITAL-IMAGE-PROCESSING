\chapter{Network Architecture and Implementation}

\section{Overview}

The handwritten digit classification problem is formulated as a supervised multi-class classification task, where the objective is to learn a discriminative mapping from high-dimensional pixel space to discrete class labels~\cite{tds_mlp_handwritten_digits}. The input space consists of grayscale images of size $28 \times 28$ pixels, which are flattened into latent representations of dimension 784~\cite{digit_eda_varianceexplained}. Each latent vector $\mathbf{x} \in \mathbb{R}^{784}$ encodes the spatial intensity distribution of a handwritten digit, where pixel values are normalized to the range $[0, 1]$.

The classification problem aims to assign each input latent vector $\mathbf{x}_i$ to one of ten digit classes $\mathcal{C} = \{0, 1, 2, \ldots, 9\}$. Given a training dataset $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$ where $y_i \in \mathcal{C}$ denotes the ground-truth label, the model learns a function $f: \mathbb{R}^{784} \rightarrow \mathbb{R}^{10}$ that maps input features to a probability distribution over classes. The predicted class is obtained via the argmax operation: $\hat{y}_i = \arg\max_k f(\mathbf{x}_i)_k$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/theory/architecture.png}
    \caption{Feed-forward neural network architecture for digit classification.}
    \label{fig:architecture}
\end{figure}

\input{content/chapter2/1-network-construction}
\input{content/chapter2/2-activation-functions}
\input{content/chapter2/3-forward-propagation}
\input{content/chapter2/4-backward-propagation}
\input{content/chapter2/5-training-process}
\input{content/chapter2/6-model-persistence}

\section{Backward Propagation}

Backward propagation computes gradients of the loss function with respect to all network parameters using the chain rule of calculus. The gradients are then used to update parameters via gradient descent, minimizing the classification error.

\subsection{Loss Function}

The network is trained using the cross-entropy loss function, which measures the discrepancy between predicted probability distributions and true class labels. For a batch of $N$ samples, the loss is defined as:

\begin{equation}
\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{10} y_{i,k} \log(\hat{y}_{i,k})
\end{equation}

where $\mathbf{y}_i \in \mathbb{R}^{10}$ is the one-hot encoded ground-truth label vector for sample $i$, and $\hat{\mathbf{y}}_i = \text{Softmax}(\mathbf{z}_2^{(i)})$ is the predicted probability distribution. The one-hot encoding ensures that $y_{i,k} = 1$ for the true class $k$ and $y_{i,k} = 0$ for all other classes.

When labels are provided as integer class indices $c_i \in \{0, 1, \ldots, 9\}$, the loss simplifies to:

\begin{equation}
\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \log(\hat{y}_{i,c_i})
\end{equation}

\subsection{Gradient Computation}

The backpropagation algorithm computes gradients layer by layer, starting from the output layer and propagating backward to the input layer.

\subsubsection{Output Layer Gradients}

The gradient of the cross-entropy loss with respect to the Softmax logits $\mathbf{Z}_2$ is computed first. For a single sample with one-hot target $\mathbf{y}$ and Softmax output $\hat{\mathbf{y}}$, the gradient is:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{z}_2} = \hat{\mathbf{y}} - \mathbf{y}
\end{equation}

For a batch of $N$ samples, this extends to:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_2} = \frac{1}{N}(\hat{\mathbf{Y}} - \mathbf{Y})
\end{equation}

where $\mathbf{Y} \in \mathbb{R}^{N \times 10}$ is the batch of one-hot encoded labels. This elegant form arises from the combination of cross-entropy loss and Softmax activation, where the gradient reduces to the difference between predictions and targets.

\subsubsection{Hidden Layer Parameter Gradients}

The gradient with respect to the output layer weights $\mathbf{W}_2$ is computed as:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}_2} = \mathbf{H}_1^\top \frac{\partial \mathcal{L}}{\partial \mathbf{Z}_2}
\end{equation}

yielding a matrix of shape $d_h \times 10$. The gradient with respect to the output bias $\mathbf{b}_2$ is:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{b}_2} = \sum_{i=1}^{N} \frac{\partial \mathcal{L}}{\partial \mathbf{z}_2^{(i)}}
\end{equation}

which sums the gradient contributions across all samples in the batch.

\subsubsection{Propagation to Hidden Layer}

The gradient is propagated backward through the hidden layer activation. The gradient with respect to the hidden pre-activations $\mathbf{Z}_1$ is:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_1} = \left(\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_2} \mathbf{W}_2^\top\right) \odot \text{ReLU}'(\mathbf{Z}_1)
\end{equation}

where $\odot$ denotes element-wise multiplication and $\text{ReLU}'(\mathbf{Z}_1)$ is the derivative of ReLU, which equals 1 where $\mathbf{Z}_1 > 0$ and 0 elsewhere.

\subsubsection{Input Layer Parameter Gradients}

The gradients with respect to the hidden layer weights and biases are:

\begin{align}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}_1} &= \mathbf{X}^\top \frac{\partial \mathcal{L}}{\partial \mathbf{Z}_1} \\
\frac{\partial \mathcal{L}}{\partial \mathbf{b}_1} &= \sum_{i=1}^{N} \frac{\partial \mathcal{L}}{\partial \mathbf{z}_1^{(i)}}
\end{align}

\subsection{Parameter Update}

After computing all gradients, parameters are updated using gradient descent with learning rate $\alpha$:

\begin{align}
\mathbf{W}_1 &\leftarrow \mathbf{W}_1 - \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{W}_1} \\
\mathbf{b}_1 &\leftarrow \mathbf{b}_1 - \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{b}_1} \\
\mathbf{W}_2 &\leftarrow \mathbf{W}_2 - \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{W}_2} \\
\mathbf{b}_2 &\leftarrow \mathbf{b}_2 - \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{b}_2}
\end{align}

\subsection{Example: Backward Pass with Small Matrices}

Continuing from the forward pass example, assume the predicted probabilities and true labels are:

\begin{align}
\hat{\mathbf{Y}} &= \begin{bmatrix}
0.15 & 0.10 & 0.20 & 0.12 & 0.08 & 0.10 & 0.05 & 0.08 & 0.10 & 0.02 \\
0.08 & 0.12 & 0.15 & 0.18 & 0.10 & 0.12 & 0.08 & 0.10 & 0.05 & 0.02
\end{bmatrix} \\
\mathbf{Y} &= \begin{bmatrix}
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\end{align}

The gradient at the output layer is:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}_2} = \frac{1}{2}(\hat{\mathbf{Y}} - \mathbf{Y}) = \begin{bmatrix}
0.075 & 0.05 & -0.4 & 0.06 & 0.04 & 0.05 & 0.025 & 0.04 & 0.05 & 0.01 \\
0.04 & 0.06 & 0.075 & -0.41 & 0.05 & 0.06 & 0.04 & 0.05 & 0.025 & 0.01
\end{bmatrix}
\end{equation}

The gradient with respect to $\mathbf{W}_2$ is computed as:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}_2} = \mathbf{H}_1^\top \frac{\partial \mathcal{L}}{\partial \mathbf{Z}_2}
\end{equation}

which involves matrix multiplication between the transposed hidden activations and the output gradient. Similar computations yield gradients for all remaining parameters.
